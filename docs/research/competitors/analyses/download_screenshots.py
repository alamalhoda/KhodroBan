#!/usr/bin/env python3
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ± Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†â€ŒÙ‡Ø§ Ø§Ø² Ù…Ø§Ø±Ú©Øª Ù¾Ù„ÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
"""

import requests
from bs4 import BeautifulSoup
import os
import time
from urllib.parse import urljoin, urlparse
import json

# User-Agent Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø´Ø¯Ù†
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ù‚Ø¨Ø§
COMPETITORS = {
    'doriyar': {
        'name': 'Ø¯ÙˆØ±ÛŒØ§Ø±',
        'cafebazaar': 'https://cafebazaar.ir/app/com.servicapp',
        'myket': 'https://myket.ir/app/com.servicapp'
    },
    'mashin-man': {
        'name': 'Ù…Ø§Ø´ÛŒÙ† Ù…Ù†',
        'cafebazaar': 'https://cafebazaar.ir/app/com.anasoftco.mycar',
        'myket': 'https://myket.ir/app/com.solu.mycar'
    },
    'khodroyar': {
        'name': 'Ø®ÙˆØ¯Ø±ÙˆÛŒØ§Ø±',
        'myket': 'https://myket.ir/app/com.serendip.carfriend.persian',
        'website': 'https://khodroyar.org/apps/khodroyar-app/'
    },
    'soupop': {
        'name': 'Ø³ÙˆÙ¾Ø§Ù¾',
        'website': 'https://soupop.ir'
    },
    'virazh': {
        'name': 'ÙˆÛŒØ±Ø§Ú˜',
        'myket': 'https://myket.ir/app/ir.virazh.owner.twa',
        'website': 'https://virazh.ir/'
    }
}

def download_image(url, save_path):
    """Ø¯Ø§Ù†Ù„ÙˆØ¯ ÛŒÚ© ØªØµÙˆÛŒØ± Ø§Ø² URL"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            with open(save_path, 'wb') as f:
                f.write(response.content)
            return True
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ {url}: {e}")
    return False

def extract_screenshots_cafebazaar(url, save_dir):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµØ§ÙˆÛŒØ± Ø§Ø² Ú©Ø§ÙÙ‡â€ŒØ¨Ø§Ø²Ø§Ø±"""
    screenshots = []
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµØ§ÙˆÛŒØ± Ø§Ø³Ú©Ø±ÛŒÙ†â€ŒØ´Ø§Øª (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ù…ØªÙØ§ÙˆØª Ø¨Ø§Ø´Ù†Ø¯)
            # Ú†Ù†Ø¯ Ø§Ù„Ú¯ÙˆÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
            patterns = [
                {'tag': 'img', 'class': 'screenshot'},
                {'tag': 'img', 'class': 'app-screenshot'},
                {'tag': 'img', 'class': 'screenshot-image'},
                {'tag': 'img', 'attrs': {'data-src': True}},
                {'tag': 'img', 'attrs': {'src': True}}
            ]
            
            for pattern in patterns:
                if 'class' in pattern:
                    imgs = soup.find_all(pattern['tag'], class_=pattern['class'])
                elif 'attrs' in pattern:
                    imgs = soup.find_all(pattern['tag'], attrs=pattern['attrs'])
                else:
                    imgs = soup.find_all(pattern['tag'])
                
                for img in imgs:
                    src = img.get('data-src') or img.get('src')
                    if src and ('screenshot' in src.lower() or 'screen' in src.lower()):
                        full_url = urljoin(url, src)
                        if full_url not in screenshots:
                            screenshots.append(full_url)
        
        # Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±
        for i, screenshot_url in enumerate(screenshots[:10]):  # Ø­Ø¯Ø§Ú©Ø«Ø± Û±Û° ØªØµÙˆÛŒØ±
            filename = f'screenshot_{i+1}.jpg'
            filepath = os.path.join(save_dir, filename)
            if download_image(screenshot_url, filepath):
                print(f"âœ“ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯: {filename}")
            time.sleep(1)  # ØªØ£Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø´Ø¯Ù†
            
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ú©Ø§ÙÙ‡â€ŒØ¨Ø§Ø²Ø§Ø±: {e}")
    
    return screenshots

def extract_screenshots_myket(url, save_dir):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµØ§ÙˆÛŒØ± Ø§Ø² Ù…Ø§ÛŒÚ©Øª"""
    screenshots = []
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµØ§ÙˆÛŒØ± Ø§Ø³Ú©Ø±ÛŒÙ†â€ŒØ´Ø§Øª
            patterns = [
                {'tag': 'img', 'class': 'screenshot'},
                {'tag': 'img', 'class': 'app-screenshot'},
                {'tag': 'img', 'attrs': {'data-src': True}},
                {'tag': 'img', 'attrs': {'src': True}}
            ]
            
            for pattern in patterns:
                if 'class' in pattern:
                    imgs = soup.find_all(pattern['tag'], class_=pattern['class'])
                elif 'attrs' in pattern:
                    imgs = soup.find_all(pattern['tag'], attrs=pattern['attrs'])
                else:
                    imgs = soup.find_all(pattern['tag'])
                
                for img in imgs:
                    src = img.get('data-src') or img.get('src')
                    if src and ('screenshot' in src.lower() or 'screen' in src.lower()):
                        full_url = urljoin(url, src)
                        if full_url not in screenshots:
                            screenshots.append(full_url)
        
        # Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±
        for i, screenshot_url in enumerate(screenshots[:10]):  # Ø­Ø¯Ø§Ú©Ø«Ø± Û±Û° ØªØµÙˆÛŒØ±
            filename = f'screenshot_{i+1}.jpg'
            filepath = os.path.join(save_dir, filename)
            if download_image(screenshot_url, filepath):
                print(f"âœ“ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯: {filename}")
            time.sleep(1)  # ØªØ£Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¨Ù„Ø§Ú© Ø´Ø¯Ù†
            
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ù…Ø§ÛŒÚ©Øª: {e}")
    
    return screenshots

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    for competitor_id, competitor_info in COMPETITORS.items():
        print(f"\n{'='*50}")
        print(f"Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {competitor_info['name']}")
        print(f"{'='*50}")
        
        competitor_dir = os.path.join(base_dir, competitor_id)
        os.makedirs(competitor_dir, exist_ok=True)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§ÙÙ‡â€ŒØ¨Ø§Ø²Ø§Ø±
        if 'cafebazaar' in competitor_info:
            print(f"\nğŸ“± Ú©Ø§ÙÙ‡â€ŒØ¨Ø§Ø²Ø§Ø±: {competitor_info['cafebazaar']}")
            extract_screenshots_cafebazaar(competitor_info['cafebazaar'], competitor_dir)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø§ÛŒÚ©Øª
        if 'myket' in competitor_info:
            print(f"\nğŸ“± Ù…Ø§ÛŒÚ©Øª: {competitor_info['myket']}")
            extract_screenshots_myket(competitor_info['myket'], competitor_dir)
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆØ¨â€ŒØ³Ø§ÛŒØª (Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯)
        if 'website' in competitor_info:
            print(f"\nğŸŒ ÙˆØ¨â€ŒØ³Ø§ÛŒØª: {competitor_info['website']}")
            # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒÙ…
        
        time.sleep(2)  # ØªØ£Ø®ÛŒØ± Ø¨ÛŒÙ† Ø±Ù‚Ø¨Ø§
    
    print(f"\n{'='*50}")
    print("âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
    print(f"{'='*50}")

if __name__ == '__main__':
    main()
